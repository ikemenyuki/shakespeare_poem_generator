{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data and create dictionary for the symbols used in shakespeare's poem\n",
    "filepath = \"poem_data/shakespeare.txt\"\n",
    "syllables_path = \"poem_data/Syllable_dictionary.txt\"\n",
    "f = open(filepath, \"r\").read()\n",
    "unique_chars = set(f.lower())\n",
    "dictionary = []\n",
    "for item in unique_chars:\n",
    "    if item not in ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']:\n",
    "        dictionary.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "syllab_df = pd.DataFrame(columns=['word', 'endtone', 'tone1', 'tone2'])\n",
    "\n",
    "with open(syllables_path) as f:\n",
    "    content = f.readlines()\n",
    "    content = [x.strip().lower() for x in content if len(x.strip().lower()) > 1]\n",
    "    i = 0\n",
    "    for line in content:\n",
    "        line = line.split(' ')\n",
    "        if len(line) == 2:\n",
    "            syllab_df.loc[i] = [line[0], '', line[1], '']\n",
    "        else:\n",
    "            if line[1][0] == 'e':\n",
    "                syllab_df.loc[i] = [line[0], line[1][1], line[2], '']\n",
    "            else:\n",
    "                syllab_df.loc[i] = [line[0], '', line[1], line[2]]\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method for encoding/decoding the strings to one-hot\n",
    "def encoding(char):\n",
    "    arr = [0 for i in range(len(dictionary))]\n",
    "    arr[dictionary.index(char)]=1\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>endtone</th>\n",
       "      <th>tone1</th>\n",
       "      <th>tone2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>'gainst</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'greeing</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'scaped</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'tis</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'twixt</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>yours</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>youth</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3202</th>\n",
       "      <td>youth's</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203</th>\n",
       "      <td>youthful</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3204</th>\n",
       "      <td>zealous</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3205 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          word endtone tone1 tone2\n",
       "0      'gainst             1      \n",
       "1     'greeing       1     2      \n",
       "2      'scaped             1      \n",
       "3         'tis             1      \n",
       "4       'twixt             1      \n",
       "...        ...     ...   ...   ...\n",
       "3200     yours             1      \n",
       "3201     youth             1      \n",
       "3202   youth's             1      \n",
       "3203  youthful             2      \n",
       "3204   zealous             2      \n",
       "\n",
       "[3205 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllab_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper method for data cleaning\n",
    "text = open(filepath, 'rb').read().decode(encoding='utf-8').lower().split('\\n\\n\\n')\n",
    "def remove_number(string):\n",
    "    return string[string.find('\\n')+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, I tried to encode the syllables. I loaded the files into a dataframe, but did not come up with a proper way to encode it and feed into the RNN model. \n",
    "\n",
    "Then, I chose to manually save all the symbols used in the training texts and saved it as my dictionary. The dictionary consists of 26 letters and other symbols like '\\n' and is 38 in length. After thinking carefully, I chose to encode it in one-hot, transforming each symbol to a tensor of dimension (38), with mostly zeros and the entry at the index where the symbol locates in the dictionary being 1. I splitted the whole text into lists of consecutive 40 symbols. Each sequence of 40 symbols will be feed into the RNN model and the next word of the sequence will be used as the target. Essentially, I was trying to use the previous sequence (40 symbols) to predict the next symbol. Hence, I had the corresponded training inputs and targets. \n",
    "\n",
    "I also tried to encode the target in the one-hot format, but when I trained it with cross entropy loss, I got an error saying that the cross entropy loss should only be applied to scalar value targets instead of one-hot encoded. Then, I tried to feed the index of the target symbol, which is a scalar value, into the RNN, but the model did not perform well. Then, I recalled that the cross entropy loss = log_softmax + nll_loss, so I added a log_softmax layer to my predicted output inside my forward function and used the nll_loss as my criterion. It turned out to have reasonably good performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customize Shakespeare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize dataset\n",
    "class ShakespeareDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, filepath):\n",
    "        text = open(filepath, 'rb').read().decode(encoding='utf-8').lower().split('\\n\\n\\n')\n",
    "        chapters = list(map(remove_number, text))\n",
    "        self.inp = []\n",
    "        self.out = []\n",
    "        \n",
    "        for i in range(len(chapters)):\n",
    "            for j in range(len(chapters[i])-40):\n",
    "                self.inp.append(list(map(encoding, chapters[i][j:j+40])))\n",
    "                self.out.append(encoding(chapters[i][j+40]))\n",
    "        \n",
    "        self.inp = torch.FloatTensor(self.inp)\n",
    "        self.out = torch.FloatTensor(self.out)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inp[idx], self.out[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespearedataset = ShakespeareDataSet(filepath)\n",
    "train_dataloader = DataLoader(shakespearedataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM neural networks\n",
    "EMBEDDING_SIZE = 64\n",
    "INP_DIMENSION = 38\n",
    "class ShakespeareLSTM(nn.Module):\n",
    "    def __init__(self, n_hidden=128, n_layers=2,\n",
    "                                   drop_prob=0.2, lr=0.001, embedding_size = EMBEDDING_SIZE):\n",
    "        super(ShakespeareLSTM, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.linear=nn.Linear(INP_DIMENSION, embedding_size)\n",
    "        # lstm layer\n",
    "        self.lstm=nn.LSTMCell(embedding_size, n_hidden)\n",
    "        #dropout layer\n",
    "        self.dropout=nn.Dropout(drop_prob)  \n",
    "        #output layer\n",
    "        self.fc=nn.Linear(n_hidden, INP_DIMENSION)\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        batch_size = inp.shape[0]\n",
    "        hidden = self.init_hidden(batch_size)\n",
    "        for i in range(inp.shape[1]):\n",
    "            single_char = inp[:, i, :]\n",
    "            single_char = F.relu(self.linear(single_char))\n",
    "            hidden = self.lstm(single_char, hidden)\n",
    "            out = F.log_softmax(self.fc(hidden[0]), dim=1)\n",
    "        return out\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h_0 = torch.zeros(batch_size, self.n_hidden).to(device)\n",
    "        c_0 = torch.zeros(batch_size, self.n_hidden).to(device)\n",
    "        nn.init.xavier_normal_(h_0)\n",
    "        nn.init.xavier_normal_(c_0)\n",
    "        return (h_0, c_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=5, lr=0.001, clip=5, val_frac=0.1, print_every=400):\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for i_batch, batch_data in enumerate(data):\n",
    "            inp, target = batch_data               \n",
    "            net.zero_grad()\n",
    "            output = net(inp)\n",
    "            loss = criterion(output, torch.max(target, 1)[1])\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            if i_batch % print_every == 0:\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}/{}...\".format(i_batch, len(train_dataloader)),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ShakespeareLSTM(\n",
      "  (linear): Linear(in_features=38, out_features=64, bias=True)\n",
      "  (lstm): LSTMCell(64, 128)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=38, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define and print the net\n",
    "n_hidden = 128\n",
    "n_layers = 2\n",
    "\n",
    "shakenet = ShakespeareLSTM(n_hidden, n_layers)\n",
    "print(shakenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2... Step: 0/1375... Loss: 1.4680...\n",
      "Epoch: 1/2... Step: 200/1375... Loss: 1.2205...\n",
      "Epoch: 1/2... Step: 400/1375... Loss: 1.2534...\n",
      "Epoch: 1/2... Step: 600/1375... Loss: 1.5665...\n",
      "Epoch: 1/2... Step: 800/1375... Loss: 1.3046...\n",
      "Epoch: 1/2... Step: 1000/1375... Loss: 1.3801...\n",
      "Epoch: 1/2... Step: 1200/1375... Loss: 1.6903...\n",
      "Epoch: 2/2... Step: 0/1375... Loss: 1.1575...\n",
      "Epoch: 2/2... Step: 200/1375... Loss: 1.5174...\n",
      "Epoch: 2/2... Step: 400/1375... Loss: 1.6945...\n",
      "Epoch: 2/2... Step: 600/1375... Loss: 1.4279...\n",
      "Epoch: 2/2... Step: 800/1375... Loss: 1.2962...\n",
      "Epoch: 2/2... Step: 1000/1375... Loss: 1.3395...\n",
      "Epoch: 2/2... Step: 1200/1375... Loss: 1.1117...\n"
     ]
    }
   ],
   "source": [
    "n_epochs =  2\n",
    "\n",
    "# train the model\n",
    "train(shakenet, train_dataloader, epochs=n_epochs, lr=0.001, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 0/1375... Loss: 1.0283...\n",
      "Epoch: 1/1... Step: 200/1375... Loss: 1.2158...\n",
      "Epoch: 1/1... Step: 400/1375... Loss: 1.5977...\n",
      "Epoch: 1/1... Step: 600/1375... Loss: 1.2830...\n",
      "Epoch: 1/1... Step: 800/1375... Loss: 1.1092...\n",
      "Epoch: 1/1... Step: 1000/1375... Loss: 0.8747...\n",
      "Epoch: 1/1... Step: 1200/1375... Loss: 1.0663...\n"
     ]
    }
   ],
   "source": [
    "# 33 epochs\n",
    "train(shakenet, train_dataloader, epochs=1, lr=0.001, print_every=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 0/1375... Loss: 1.0709...\n",
      "Epoch: 1/1... Step: 200/1375... Loss: 1.0228...\n",
      "Epoch: 1/1... Step: 400/1375... Loss: 1.6351...\n",
      "Epoch: 1/1... Step: 600/1375... Loss: 1.3580...\n",
      "Epoch: 1/1... Step: 800/1375... Loss: 1.0721...\n",
      "Epoch: 1/1... Step: 1000/1375... Loss: 1.1174...\n",
      "Epoch: 1/1... Step: 1200/1375... Loss: 1.1743...\n"
     ]
    }
   ],
   "source": [
    "# 35 epochs\n",
    "train(shakenet, train_dataloader, epochs=1, lr=0.001, print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after training for 35 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freestyle(net, string, temperature):\n",
    "    predicted = ''\n",
    "    output = ''\n",
    "    poem_length = 13 * 40\n",
    "    net.eval()\n",
    "    pre = string\n",
    "    for i in range(poem_length):\n",
    "        inp = torch.FloatTensor([list(map(encoding, string))])\n",
    "        out= shakenet(inp)\n",
    "         # Apply temperature\n",
    "        soft_out = F.softmax(out / temperature, dim=1)\n",
    "        p = soft_out.data.cpu().numpy()\n",
    "        idx = np.random.choice(out.size()[1], p=p[0])\n",
    "        predicted = dictionary[idx]\n",
    "        string = string[1:] + predicted\n",
    "        output += predicted\n",
    "    return pre + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'shakenet.net'\n",
    "\n",
    "checkpoint = {'n_hidden': shakenet.n_hidden,\n",
    "              'n_layers': shakenet.n_layers,\n",
    "              'state_dict': shakenet.state_dict()}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemented a Char-LSTM model together with linear layers. First I tried to embed the input tensor into a bigger embedded tensor, passing it to a lstm model, and then applied the log_softmax function on the final output. I tunned the n_hidden (number of hidden units in a LSTM cell) the embedding_size for the embedded layer. After trainning for 35 epochs, the loss of my model converges around 1.1. The model successfully learns the wording of Shakespearen poems (using words like thou, art, hath, thee, etc...), most of the English standard words, poem structures, sentence structure, and some of English grammars. However, it does not learn the sonnet structure. With Regard to training time, it took approximately 30 minutes to train for 30 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "in oftain afformiring silfulloke\n",
      "shablow upituves the tail hold will to thee,\n",
      "when ftering sending now is fwailttend.\n",
      "'win i'ld their read or shore tiffured plide,\n",
      "  heis tatten teach acant outcost anone fast.\n",
      "so trem thy vilents of like be fornare;\n",
      "childot uppat on hadoure my flame gok:\n",
      "lively he alone hope blind,\n",
      "of hid, ere that leapes this is, bitrence dottly.\n",
      "  onlying: leppable th' outwant smel.\n",
      "werticold my sweet sowers to stilage huse freemen!\n",
      "more and outwogh to rememfors gateoun.\n",
      "wo diliel a waudeusy eye,\n"
     ]
    }
   ],
   "source": [
    "# using temperature of 1.5\n",
    "poem_start = \"shall i compare thee to a summer's day?\\n\"\n",
    "print(freestyle(shakenet, poem_start, 1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "whose thou which still were holying inwart's pride hand,\n",
      "and all alone alone outlive her false,\n",
      "and the mounten more ill revosten fitted,\n",
      "for sweet thine eye a view, the world love'\n",
      "to time and the parton complexioned flobe,\n",
      "whilst my selfouty shall you thine eyele,\n",
      "steal still my day so sleep to the sweet,\n",
      "but all the world and the rich from kning.\n",
      "  but that thou my love in his self a look,\n",
      "what come to make the life to live to hope.\n",
      "  yet in thy furpoil on the summer ingst.\n",
      "  but yet not i dring the treamperouse\n"
     ]
    }
   ],
   "source": [
    "# using temperature of 0.75\n",
    "print(freestyle(shakenet, poem_start, 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "  then from the world's looks to the world doth stand\n",
      "o that still my self the bearth the world be.\n",
      "  then thou thy self the world with the rank,\n",
      "and therefore to be world with the world to decay,\n",
      "and that i see the mounten the most doth the store,\n",
      "  then i am sometime where the world and there,\n",
      "the summer's present speak of the world heart,\n",
      "which i am the praise the world with from thee,\n",
      "  then thou art a self the world with thee.\n",
      "  for the world and there in the world with thee.\n",
      "  you with the world and soul whil\n"
     ]
    }
   ],
   "source": [
    "# using temperature of 0.25\n",
    "print(freestyle(shakenet, poem_start, 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing poem generations with different temperature, I oberserve that the smaller the temperature, the more accurate (more English-like, Shakespeare-like) poems are generated. For example, the poems using temperature=0.25 makes more sense than those using 0.75 and 1.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In problem 2 I was using a stateless LSTM model with nn.LSTMCell model, I am going to improve my model using a stateful LSTM model with nn.LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM neural networks\n",
    "class ShakespeareLSTM2(nn.Module):\n",
    "    def __init__(self, n_hidden=128, n_layers=2,\n",
    "                                   drop_prob=0.2, lr=0.001):\n",
    "        super(ShakespeareLSTM2, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        self.seq_length = 40\n",
    "        # lstm layer\n",
    "        self.lstm=nn.LSTM(38, n_hidden, n_layers,\n",
    "                    batch_first=True)\n",
    "        #dropout layer\n",
    "        self.dropout=nn.Dropout(drop_prob)  \n",
    "        #output layer\n",
    "        self.fc=nn.Linear(n_hidden * self.seq_length, 38)\n",
    "        \n",
    "    def forward(self, inp, hidden):\n",
    "        r_output, hidden = self.lstm(inp, hidden)\n",
    "        out = self.dropout(r_output)\n",
    "        # Stack up LSTM outputs using view\n",
    "        out = out.contiguous().view(len(out), self.n_hidden * out.shape[1])\n",
    "        ## put x through the fully-connected layer\n",
    "        out = F.log_softmax(self.fc(out), dim=1)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(net, data, epochs=5, batch_size=64, lr=0.001, seq_length=40, clip=5, val_frac=0.1, print_every=400):\n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        prev_batch_size = 0\n",
    "        h = net.init_hidden(batch_size)\n",
    "        for i_batch, batch_data in enumerate(data):\n",
    "            inp, target = batch_data\n",
    "            \n",
    "            #handling varying batch size at the end of epoch\n",
    "            if prev_batch_size != len(inp):\n",
    "                h = net.init_hidden(len(inp))\n",
    "                prev_batch_size = len(inp)\n",
    "                \n",
    "            h = tuple([each.data for each in h])\n",
    "            net.zero_grad()\n",
    "            output, h = net(inp, h)\n",
    "            loss = criterion(output, torch.max(target, 1)[1])\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "            if i_batch % print_every == 0:\n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}/{}...\".format(i_batch, len(train_dataloader)),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freestyle2(net, string, temperature):\n",
    "    predicted = ''\n",
    "    output = ''\n",
    "    poem_length = 13 * 40\n",
    "    net.eval()\n",
    "    pre = string\n",
    "    for i in range(poem_length):\n",
    "        inp = torch.FloatTensor([list(map(encoding, string))])\n",
    "        h = net.init_hidden(1)\n",
    "        h = tuple([each.data for each in h])\n",
    "        out, h = net(inp, h)\n",
    "         # Apply temperature\n",
    "        soft_out = F.softmax(out / temperature, dim=1)\n",
    "        p = soft_out.data.cpu().numpy()\n",
    "        idx = np.random.choice(out.size()[1], p=p[0])\n",
    "        predicted = dictionary[idx]\n",
    "        string = string[1:] + predicted\n",
    "        output += predicted\n",
    "    return pre + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 128\n",
    "n_layers = 2\n",
    "shakenet2 = ShakespeareLSTM2(n_hidden, n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## after training for 40 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1... Step: 0/1375... Loss: 0.3315...\n",
      "Epoch: 1/1... Step: 200/1375... Loss: 0.3223...\n",
      "Epoch: 1/1... Step: 400/1375... Loss: 0.6391...\n",
      "Epoch: 1/1... Step: 600/1375... Loss: 0.3848...\n",
      "Epoch: 1/1... Step: 800/1375... Loss: 0.5677...\n",
      "Epoch: 1/1... Step: 1000/1375... Loss: 0.4695...\n",
      "Epoch: 1/1... Step: 1200/1375... Loss: 0.3744...\n"
     ]
    }
   ],
   "source": [
    "n_epochs =  1\n",
    "\n",
    "# train the model for 40 epochs\n",
    "train2(shakenet2, train_dataloader, epochs=n_epochs, lr=0.001, print_every=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After being trained for 40 epochs, the loss of the new model converges around 0.3. The training loss of the improved model is significantly better than the model in problem 2. Now I try to make poems with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "thou art more lovy hials soingess, and warsettenst,\n",
      "and perspist whibe frcenothers tell.\n",
      "  but then such creatute time you a wair,\n",
      "thus fresher in meriem well thou ever which doth days.\n",
      "but deach fads in plorded mhere at beederily:\n",
      "do if borthle thal fair so him had in the lick of eie,\n",
      "and for this thou redair shines me old lends,\n",
      "nor all trought in ohe can there for trought,\n",
      "  do sheet this worths were, your self not,\n",
      "craes wo the learned mimubed threit care beof,\n",
      "and see pith't creist loow umboth their epter the,\n"
     ]
    }
   ],
   "source": [
    "# using temperature of 1.5\n",
    "poem_start = \"shall i compare thee to a summer's day?\\n\"\n",
    "print(freestyle2(shakenet2, poem_start, 1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "thou art more lovely and more temperate:\n",
      "rough winds doot come death mine eye are old,\n",
      "from false present, thou hame be secterse to. hear spirit,\n",
      "although they being sprengse thee my recais,\n",
      "of thy severing old pattech, that thus steme,\n",
      "the beauty swift diecharned of the fair,\n",
      "by to is the eityed graces benedy bying,\n",
      "when sab hom thine eyes they hear in thee frebresting with resember of eye,\n",
      "when o hemp wouk thy sweet heat gied stone,\n",
      "uprobed more, that the envermed after lave your merit.\n",
      "chelk he mistrions no true\n"
     ]
    }
   ],
   "source": [
    "# using temperature of 0.75\n",
    "poem_start = \"shall i compare thee to a summer's day?\\n\"\n",
    "print(freestyle2(shakenet2, poem_start, 0.75))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "thou art more lovely and more never tknowed when my heart:\n",
      "when i be awner doth lops an beem without,\n",
      "that find my near in their curs of thee,\n",
      "when i besond with steeling to have hat,\n",
      "to decarined from thee finst a conkers\n",
      "to that are grest and deserving with thy away,\n",
      "and art leisured thee dead beseater,\n",
      "but that i am flew of you ride and in not,\n",
      "nor that they reep usus with thy will.\n",
      "  i wit a weating my side, against my self,\n",
      "  but since what is anind in thy feasings have,\n",
      "  i please there not the oft their grow\n"
     ]
    }
   ],
   "source": [
    "# using temperature of 0.25\n",
    "poem_start = \"shall i compare thee to a summer's day?\\n\"\n",
    "print(freestyle2(shakenet2, poem_start, 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'shakenet2.net'\n",
    "\n",
    "checkpoint = {'n_hidden': shakenet2.n_hidden,\n",
    "              'n_layers': shakenet2.n_layers,\n",
    "              'state_dict': shakenet2.state_dict()}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open('shakenet2.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "model = ShakespeareLSTM2(n_hidden =checkpoint['n_hidden'],n_layers=checkpoint['n_layers'])\n",
    "model.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shall i compare thee to a summer's day?\n",
      "thou art more lovely and more temperate:\n",
      "rough winds do shake the darrion buds a perted,\n",
      "beauty's trom thy sair, can one out from thine:\n",
      "eetoed to that depart oll are of sweet wrobn,\n",
      "and own me thought in this alone, and see thoughts,\n",
      "and praise to me will point did eyes be.\n",
      "  if thou thy self despite these eyes withered,\n",
      "made show men minds what the comment of well beseet,\n",
      "the it to they are will, though thou art some ticely sake\n",
      "tell of me greef tay will doot with their\n",
      "and beloved new flook find be dear yourre,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(freestyle2(model, poem_start, 0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to improve my model using a pytorch-optimized LSTM model which also has dropout integrated in contrast to implementing the LSTM model using a for loop and pytorch LSTMCell module in problem 2. I also added a dropout layer with the probability of 0.2. The loss dropped significantly compared to the previous model. After being trained for 40 epochs, the loss of the new model converges around 0.3. The training loss of the improved model is significantly better than the model in problem 2.\n",
    "\n",
    "In terms of predictions, using temperature of 0.25, the model successfully predict the consecutive 69 letters and symbols after the given line (\"thou art more lovely and more temperate:\\n rough winds do shake the dar\"). Using other temperatures, the new model also had more English-like and Shakespeare-like performance. It performs slightly better than the previous model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
